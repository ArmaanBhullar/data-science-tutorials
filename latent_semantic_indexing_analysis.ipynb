{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author: Armaan Bhullar\n",
    "### Email: jhaniria@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook explains the basic of using LSI (Latent Semantic Indexing) techniques for:\n",
    "* Topic Modelling\n",
    "* Creating a simple searchable index over documents\n",
    "* Similarity match between documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LsiModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_names = [\"A dive into maths\",\"Advanced maths\",\"Common probability distributions\",\"The maths of probability\", \\\n",
    "              \"Stochastic probability maths\", \"Was Thanos right?\", \"Stark being stark !\",\n",
    "              \"A study of marvel characters: Stark vs Dr. Strange\", \"Why is Dr. strange so strange ?\"]\n",
    "# The above list contains books from primarily 3 topics: Maths, Probability and Marvel universe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing - \n",
    "* tokenization\n",
    "* lower string\n",
    "* Stop word removal - Not implemented here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a', 'dive', 'into', 'maths'], ['advanced', 'maths'], ['common', 'probability', 'distributions'], ['the', 'maths', 'of', 'probability'], ['stochastic', 'probability', 'maths'], ['was', 'thanos', 'right?'], ['stark', 'being', 'stark', '!'], ['a', 'study', 'of', 'marvel', 'characters:', 'stark', 'vs', 'dr.', 'strange'], ['why', 'is', 'dr.', 'strange', 'so', 'strange', '?']]\n"
     ]
    }
   ],
   "source": [
    "book_names_split = [[word.lower() for word in doc.split(\" \")] for doc in book_names]\n",
    "##You can use a very advanced tokenizer here from gensim/nltk ...\n",
    "print(book_names_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start by creating:\n",
    "* Dictionary: Basically assigns an ID to each word\n",
    "* Corpora: Uses the ID to create a mathematical representation of the documents, called corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary :  {'a': 0, 'dive': 1, 'into': 2, 'maths': 3, 'advanced': 4, 'common': 5, 'distributions': 6, 'probability': 7, 'of': 8, 'the': 9, 'stochastic': 10, 'right?': 11, 'thanos': 12, 'was': 13, '!': 14, 'being': 15, 'stark': 16, 'characters:': 17, 'dr.': 18, 'marvel': 19, 'strange': 20, 'study': 21, 'vs': 22, '?': 23, 'is': 24, 'so': 25, 'why': 26}\n",
      "Corpus:  [(0, 1), (1, 1), (2, 1), (3, 1)]\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.dictionary.Dictionary(book_names_split)\n",
    "print(\"Dictionary : \", dictionary.token2id)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in book_names_split]\n",
    "print(\"Corpus: \", corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we train a model of LSI class, the created corpora and dictionary are common for all basic Gensim models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now let's fit our model\n",
    "model = LsiModel(corpus=corpus, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main features:\n",
    "* A topic is composed of a weighted collection of words\n",
    "* Hiigher weight => More contribution to topic\n",
    "* Negative weights are natural, implyingnegative correlation with the topic\n",
    "* Not all topics are meaningful, the top topics are more meaningful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.560*\"strange\" + 0.383*\"dr.\" + 0.333*\"stark\" + 0.238*\"of\" + 0.236*\"a\" + 0.206*\"vs\" + 0.206*\"characters:\" + 0.206*\"marvel\" + 0.206*\"study\" + 0.177*\"so\"'),\n",
       " (1,\n",
       "  '-0.481*\"maths\" + -0.348*\"probability\" + 0.326*\"strange\" + -0.313*\"stark\" + -0.255*\"of\" + -0.215*\"a\" + 0.209*\"is\" + 0.209*\"why\" + 0.209*\"?\" + 0.209*\"so\"'),\n",
       " (2,\n",
       "  '0.556*\"stark\" + -0.465*\"maths\" + -0.367*\"probability\" + 0.228*\"!\" + 0.228*\"being\" + -0.182*\"strange\" + -0.150*\"the\" + -0.142*\"stochastic\" + -0.141*\"why\" + -0.141*\"is\"'),\n",
       " (3,\n",
       "  '-0.398*\"stark\" + 0.332*\"a\" + -0.320*\"!\" + -0.320*\"being\" + -0.277*\"probability\" + 0.241*\"vs\" + 0.241*\"marvel\" + 0.241*\"study\" + 0.241*\"characters:\" + 0.184*\"of\"'),\n",
       " (4,\n",
       "  '0.433*\"probability\" + -0.375*\"dive\" + -0.375*\"into\" + -0.345*\"maths\" + 0.270*\"common\" + 0.270*\"distributions\" + -0.256*\"a\" + 0.248*\"of\" + -0.133*\"advanced\" + 0.129*\"the\"')]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.show_topics(5,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's do some topic analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given a new book title,\n",
    "* preprocess in the same way as training data \n",
    "* create a bow representation using same dictionary as the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"Elementary maths\"\n",
    "doc_preprocessed = [word.lower() for word in doc.split(\" \")]\n",
    "doc_bow = dictionary.doc2bow(doc_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The per topic match of document: [(0, 0.07998423196480817), (1, -0.4814256385830975), (2, -0.4654984031690677), (3, -0.10482334268432869), (4, -0.34526711553703254), (6, -0.21329975586071215), (7, -0.19778523223281436), (8, -0.10654996870877514)]\n",
      "Max match with the following topic : [('strange', 0.5604572621811904), ('dr.', 0.38334018270966697), ('stark', 0.33326651589548706), ('of', 0.23801568143827062), ('a', 0.23637241806558176), ('vs', 0.2062231032381434), ('characters:', 0.2062231032381434), ('marvel', 0.20622310323814338), ('study', 0.20622310323814338), ('so', 0.1771170794715234)]\n",
      "[('stark', 0.5559415579606246), ('maths', -0.46549840316906793), ('probability', -0.3673124601702952), ('!', 0.22799864355803423), ('being', 0.22799864355803418), ('strange', -0.18247849776964653), ('the', -0.15027891565685814), ('stochastic', -0.14171380857932003), ('why', -0.14121138430710156), ('is', -0.14121138430710153)]\n"
     ]
    }
   ],
   "source": [
    "per_topic_match = model[doc_bow]\n",
    "print(\"The per topic match of document: {}\".format(per_topic_match))\n",
    "matched_topic = sorted( per_topic_match, key=lambda x:x[1],  reverse=True)[0][0]\n",
    "print(\"Max match with the following topic : {}\".format(model.show_topic(matched_topic)))\n",
    "print(model.show_topic(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's create a searchable index and run a query for a docA against it to find most smilar existing doc:\n",
    "1. Create index of existing documents\n",
    "2. Get the per topic match of the docA\n",
    "3. Run the index on the per_topic_match of docA in step2 above, this returns similarity scores across existing documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now let's create an index \n",
    "from gensim import similarities\n",
    "index = similarities.MatrixSimilarity(model[corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A dive into maths 0.60588354\n",
      "Advanced maths 0.8568487\n",
      "Common probability distributions 9.313226e-09\n",
      "The maths of probability 0.60588354\n",
      "Stochastic probability maths 0.6996141\n",
      "Was Thanos right? 0.0\n",
      "Stark being stark ! -1.3591489e-08\n",
      "A study of marvel characters: Stark vs Dr. Strange -3.434252e-09\n",
      "Why is Dr. strange so strange ? -1.7462298e-09\n"
     ]
    }
   ],
   "source": [
    "for book_name, match in zip(book_names, index[per_topic_match]):\n",
    "    print(book_name, match)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

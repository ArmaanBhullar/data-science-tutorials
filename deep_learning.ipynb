{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "'''Sequence to sequence example in Keras (character-level).\n",
    "This script demonstrates how to implement a basic character-level\n",
    "sequence-to-sequence model. We apply it to translating\n",
    "short English sentences into short French sentences,\n",
    "character-by-character. Note that it is fairly unusual to\n",
    "do character-level machine translation, as word-level\n",
    "models are more common in this domain.\n",
    "# Summary of the algorithm\n",
    "- We start with input sequences from a domain (e.g. English sentences)\n",
    "    and corresponding target sequences from another domain\n",
    "    (e.g. French sentences).\n",
    "- An encoder LSTM turns input sequences to 2 state vectors\n",
    "    (we keep the last LSTM state and discard the outputs).\n",
    "- A decoder LSTM is trained to turn the target sequences into\n",
    "    the same sequence but offset by one timestep in the future,\n",
    "    a training process called \"teacher forcing\" in this context.\n",
    "    Is uses as initial state the state vectors from the encoder.\n",
    "    Effectively, the decoder learns to generate `targets[t+1...]`\n",
    "    given `targets[...t]`, conditioned on the input sequence.\n",
    "- In inference mode, when we want to decode unknown input sequences, we:\n",
    "    - Encode the input sequence into state vectors\n",
    "    - Start with a target sequence of size 1\n",
    "        (just the start-of-sequence character)\n",
    "    - Feed the state vectors and 1-char target sequence\n",
    "        to the decoder to produce predictions for the next character\n",
    "    - Sample the next character using these predictions\n",
    "        (we simply use argmax).\n",
    "    - Append the sampled character to the target sequence\n",
    "    - Repeat until we generate the end-of-sequence character or we\n",
    "        hit the character limit.\n",
    "# Data download\n",
    "English to French sentence pairs.\n",
    "http://www.manythings.org/anki/fra-eng.zip\n",
    "Lots of neat sentence pairs datasets can be found at:\n",
    "http://www.manythings.org/anki/\n",
    "# References\n",
    "- Sequence to Sequence Learning with Neural Networks\n",
    "    https://arxiv.org/abs/1409.3215\n",
    "- Learning Phrase Representations using\n",
    "    RNN Encoder-Decoder for Statistical Machine Translation\n",
    "    https://arxiv.org/abs/1406.1078\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 10000  # Number of samples to train on.\n",
    "# Path to the data txt file on disk.\n",
    "data_path = '/home/armaan/Desktop/projects/data/fra-eng/fra.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10000\n",
      "Number of unique input tokens: 69\n",
      "Number of unique output tokens: 93\n",
      "Max sequence length for inputs: 16\n",
      "Max sequence length for outputs: 59\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.9203 - val_loss: 0.9563\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.7363 - val_loss: 0.7843\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.6266 - val_loss: 0.7012\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.5704 - val_loss: 0.6502\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.5319 - val_loss: 0.6164\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.4988 - val_loss: 0.5981\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.4715 - val_loss: 0.5547\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.4475 - val_loss: 0.5377\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.4275 - val_loss: 0.5211\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.4093 - val_loss: 0.5139\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.3932 - val_loss: 0.4988\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.3779 - val_loss: 0.4882\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.3638 - val_loss: 0.4773\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.3507 - val_loss: 0.4786\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.3388 - val_loss: 0.4666\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.3271 - val_loss: 0.4601\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.3160 - val_loss: 0.4615\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.3056 - val_loss: 0.4606\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.2957 - val_loss: 0.4592\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.2862 - val_loss: 0.4536\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.2770 - val_loss: 0.4576\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.2684 - val_loss: 0.4542\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.2602 - val_loss: 0.4557\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.2522 - val_loss: 0.4597\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.2447 - val_loss: 0.4572\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.2374 - val_loss: 0.4594\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.2305 - val_loss: 0.4601\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.2238 - val_loss: 0.4687\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.2174 - val_loss: 0.4680\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.2115 - val_loss: 0.4717\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.2055 - val_loss: 0.4779\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.2000 - val_loss: 0.4783\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.1940 - val_loss: 0.4810\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.1893 - val_loss: 0.4907\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.1842 - val_loss: 0.4911\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.1794 - val_loss: 0.4977\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.1749 - val_loss: 0.5013\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.1704 - val_loss: 0.5086\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.1661 - val_loss: 0.5111\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.1620 - val_loss: 0.5117\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.1580 - val_loss: 0.5269\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.1543 - val_loss: 0.5220\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.1507 - val_loss: 0.5258\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.1470 - val_loss: 0.5297\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.1436 - val_loss: 0.5354\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.1402 - val_loss: 0.5447\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.1372 - val_loss: 0.5414\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.1342 - val_loss: 0.5515\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.1315 - val_loss: 0.5490\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.1284 - val_loss: 0.5574\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.1259 - val_loss: 0.5611\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.1232 - val_loss: 0.5736\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.1209 - val_loss: 0.5749\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.1183 - val_loss: 0.5757\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.1160 - val_loss: 0.5793\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.1136 - val_loss: 0.5857\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.1114 - val_loss: 0.5904\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.1097 - val_loss: 0.5970\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.1076 - val_loss: 0.6060\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.1055 - val_loss: 0.6039\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.1034 - val_loss: 0.6147\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.1018 - val_loss: 0.6146\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.0999 - val_loss: 0.6144\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.0976 - val_loss: 0.6249\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.0962 - val_loss: 0.6221\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.0949 - val_loss: 0.6356\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.0930 - val_loss: 0.6359\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.0912 - val_loss: 0.6448\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.0901 - val_loss: 0.6429\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.0884 - val_loss: 0.6483\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.0869 - val_loss: 0.6497\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.0854 - val_loss: 0.6588\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.0841 - val_loss: 0.6593\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.0831 - val_loss: 0.6652\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.0814 - val_loss: 0.6695\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.0802 - val_loss: 0.6703\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.0790 - val_loss: 0.6739\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.0775 - val_loss: 0.6741\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.0764 - val_loss: 0.6874\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.0757 - val_loss: 0.6874\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.0742 - val_loss: 0.6910\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.0731 - val_loss: 0.6951\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.0720 - val_loss: 0.7014\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.0712 - val_loss: 0.7023\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.0699 - val_loss: 0.7055\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.0688 - val_loss: 0.7076\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.0679 - val_loss: 0.7090\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.0668 - val_loss: 0.7191\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.0660 - val_loss: 0.7217\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.0654 - val_loss: 0.7198\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.0643 - val_loss: 0.7332\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.0634 - val_loss: 0.7275\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.0628 - val_loss: 0.7275\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.0615 - val_loss: 0.7280\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.0611 - val_loss: 0.7323\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.0602 - val_loss: 0.7363\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.0594 - val_loss: 0.7382\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.0585 - val_loss: 0.7455\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.0577 - val_loss: 0.7525\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.0572 - val_loss: 0.7518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/armaan/.local/share/virtualenvs/Desktop-9TTn8RCB/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "# Save model\n",
    "model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f22492fd6a0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 460.8x403.2 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from jupyterthemes import jtplot\n",
    "jtplot.style('onedork')\n",
    "import seaborn as sns\n",
    "sns.set_style()\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(model.history.history['loss'], label='loss')\n",
    "plt.plot(model.history.history['val_loss'], label='val_loss')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named tensorflow.python.client",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-48501209a23d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdevice_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named tensorflow.python.client"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
